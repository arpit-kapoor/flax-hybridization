{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybridizing process-based models with ML using Jax and Flax \n",
    "\n",
    "Arpit Kapoor 2023\n",
    "\n",
    "In this notebook, we will first train a process-based hydrological model (Simple AWBM) on synthetic streamflow data and then use a Multi-layered perceptron to train hybrid streamflow prediction model.\n",
    "\n",
    "You can pull the docker image from docker hub with the following command:\n",
    "```bash\n",
    "docker pull jsimdare/darenumpyro\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start import by necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp                # For numpy operations in jax\n",
    "import optax                           # Optimization package for jax\n",
    "\n",
    "from tqdm import tqdm                  # To print progress bar\n",
    "\n",
    "from flax import linen as nn\n",
    "from clu import metrics                # To keep track of training metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read synthetic streamflow data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\n",
    "    os.path.join(\n",
    "        'project', 'data', 'rr_Data.csv'\n",
    "    ), index_col=0, parse_dates=True\n",
    ")\n",
    "raw_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract features and targets as jax device arrays from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = jnp.array(raw_data['prec'].values) # precipitation\n",
    "etp = jnp.array(raw_data['etp'].values) # evaporation\n",
    "q_obs = jnp.array(raw_data['qobs'].values).reshape(-1, 1) # observed discharge\n",
    "q_date = raw_data.index.values # date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks\n",
    "print('Precipitation: ', prec.shape)\n",
    "print('Evaporation: ', etp.shape)\n",
    "print('Observed discharge: ', q_obs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create covariate and target vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = jnp.stack([prec,etp], axis=1)\n",
    "targets = q_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Generators\n",
    "root_key = jax.random.PRNGKey(seed=1)\n",
    "main_key, params_key = jax.random.split(key=root_key, num=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate and train the Process-based model (AWBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.awbm import SimpleAWBM\n",
    "\n",
    "awbm = SimpleAWBM(\n",
    "    S_init=10.0,\n",
    "    B_init=10.0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataclass for keeping track of metrics during the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Define metrics to track during training\"\"\"\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "    nse: metrics.Average.from_output('nse')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flax is designed to keep the model states independent of the module object. This is done via TrainState object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    \"\"\"Train state stores the current state of model parameter and optimizer\n",
    "       We also store the current metrics and random number generator keys\n",
    "    \"\"\"\n",
    "    metrics: Metrics\n",
    "    key: jax.random.KeyArray\n",
    "\n",
    "def create_train_state(module: nn.Module, main_key, params_key, lr, n_features):\n",
    "    \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "\n",
    "    # TODO: Initialise model parameters\n",
    "\n",
    "    # TODO: Create optimizer instance\n",
    "\n",
    "    # TODO: Create TrainState instance\n",
    "\n",
    "    return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = create_train_state(awbm, \n",
    "                                 main_key=main_key, \n",
    "                                 params_key=params_key, \n",
    "                                 lr=1e-2,\n",
    "                                 n_features=covariates.shape[-1])\n",
    "\n",
    "model_state.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the `train_step` function to execute one training step on single batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch, targets):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        # TODO: Define the function to compute MSE loss\n",
    "        loss = \n",
    "        return loss\n",
    "    \n",
    "    # Function to compute gradients of the loss_fn\n",
    "    grad_fn = jax.grad(loss_fn)\n",
    "\n",
    "    # TODO: Compute and apply gradient to the train state\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Nash–Sutcliffe Efficiency (NSE) coefficient to evaluate the streamflow predictions\n",
    "\n",
    "$$\n",
    "NSE = 1 - \\frac{\\sum_j (Q_j - \\hat{Q}_j)^2}{\\sum_j (Q_j - \\bar{Q})^2}\n",
    "$$\n",
    "\n",
    "where $Q$ is the observed streamflow and $\\hat{Q}$ is the predicted stream flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def nse(targets: jnp.ndarray, predictions: jnp.ndarray):\n",
    "    \"\"\"Function to compute Nash–Sutcliffe Efficiency (NSE) coefficient\"\"\"\n",
    "    numer = jnp.sum(jnp.square(targets-predictions))\n",
    "    denom = jnp.sum(jnp.square(targets-jnp.mean(targets)))\n",
    "    nse_score = 1 - numer/denom\n",
    "    return nse_score\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state: TrainState, \n",
    "                    batch: jnp.ndarray, \n",
    "                    targets: jnp.ndarray):\n",
    "    \"\"\"Function to compute training metrics at each epoch\"\"\"\n",
    "\n",
    "    # Generate model prediction from state\n",
    "    preds = state.apply_fn({'params': state.params}, batch)\n",
    "    \n",
    "    # Compute loss and other metrics\n",
    "    loss = optax.l2_loss(preds, targets).mean()\n",
    "    nse_score = nse(targets, preds)\n",
    "    \n",
    "    # TODO: Compute metric updates and merge the updates\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss,\n",
    "                                                            nse=nse_score)\n",
    "    \n",
    "    # Update state\n",
    "    state = state.replace(metrics=metrics)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_metrics(metrics_history: dict, figsize: Tuple[int, int]=(10, 4)):\n",
    "    \"\"\"Utility function to plot performance metrics after training\"\"\"\n",
    "\n",
    "    # Extract the list of metrics from dict\n",
    "    metric_list = metrics_history.keys()\n",
    "\n",
    "    # Crreate subplots\n",
    "    fig, ax = plt.subplots(1, len(metric_list), figsize=figsize)\n",
    "\n",
    "    for idx, metric in enumerate(metric_list):\n",
    "        \n",
    "        # Fetch  value of current metric\n",
    "        metric_val = jnp.array(metrics_history[metric])\n",
    "        \n",
    "        # Plot the metric\n",
    "        ax[idx].plot(metric_val, color='black')\n",
    "        ax[idx].set_xlabel('Epoch')\n",
    "        ax[idx].set_ylabel(metric)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(state: TrainState, \n",
    "          covariates: jnp.ndarray, \n",
    "          targets: jnp.ndarray, \n",
    "          n_epoch: int):\n",
    "    \"\"\"Train function for training Flax modules\"\"\"\n",
    "    \n",
    "    # Progress bar to monitor training progress\n",
    "    pbar = tqdm(range(1, n_epoch+1))\n",
    "\n",
    "    # Dictionary to store training metrics at each epoch\n",
    "    metrics_history = {'loss': [], 'nse': []}\n",
    "\n",
    "    # Train Module\n",
    "    for epoch in pbar:\n",
    "\n",
    "        # Take one training step\n",
    "        state = train_step(state, covariates, targets)\n",
    "\n",
    "        # Compute metrics and update state\n",
    "        state = compute_metrics(state=state, batch=covariates, targets=targets)\n",
    "        \n",
    "        # Store metrics\n",
    "        for metric, value in state.metrics.compute().items():\n",
    "            metrics_history[metric].append(value)\n",
    "        \n",
    "        # Reset metrics of the state\n",
    "        state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "        # Print progress\n",
    "        pbar.set_description(f\"\"\"Epoch {epoch}/{n_epoch} loss: {metrics_history['loss'][-1]:.4f} NSE: {metrics_history['nse'][-1]:.4f}\"\"\")\n",
    "\n",
    "    return state, metrics_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(module: nn.Module, \n",
    "             covariates: jnp.ndarray, \n",
    "             targets: jnp.ndarray, \n",
    "             state: TrainState=None,\n",
    "             params: nn.FrozenDict=None):\n",
    "    \"\"\"Function to evaluate the model predictions\"\"\"\n",
    "    \n",
    "    # Set params and state variables for param  values to use\n",
    "    if params is None:\n",
    "        if state is None:\n",
    "            raise(\"No params provided!\")\n",
    "        params = state.params\n",
    "    \n",
    "    # Forward pass through model\n",
    "    preds = module.apply({'params': params}, covariates)\n",
    "\n",
    "    # Compute the nse score\n",
    "    nse_score = nse(preds, targets)\n",
    "\n",
    "    # Plot hydrograph\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(covariates[:, 0], 'g--', label='precip', alpha=0.40)\n",
    "    ax.plot(covariates[:, 1], 'y--', label='etp', alpha=0.30)\n",
    "    ax.plot(targets, color='black', label='obs', alpha=1.0)\n",
    "    ax.plot(preds, color='red', label='pred', alpha=0.75)\n",
    "\n",
    "    ax.set_xlabel('Timestep')\n",
    "    ax.set_ylabel('Flow (mm/day)')\n",
    "\n",
    "    ax.annotate(f'NSE: {nse_score:.4f}',\n",
    "            xy=(0.8, 0.95), xycoords='figure fraction',\n",
    "            horizontalalignment='right', verticalalignment='top',\n",
    "            fontsize=12)\n",
    "    ax.set_title('Streamflow prediction')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the AWBM model on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state, metrics_history = train(model_state, \n",
    "                                       covariates, targets, \n",
    "                                       n_epoch=150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance (NSE) and plot hydrograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(awbm, covariates, q_obs, state=trained_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Hybrid Model\n",
    "\n",
    "Now that we have optimised an AWBM model for this data, we will now look at integrating this with an MLP to create a hybrid model. this model takes the output of AWBM and combines it with the covariates to predict streamflow at each timestep. We will optimise for the MLP coefficients and the AWBM coefficients simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.hybrid import HybridAWBM\n",
    "\n",
    "hybrid_awbm = HybridAWBM(S_init=10.,\n",
    "                         B_init=10.,\n",
    "                         n_layers=2,\n",
    "                         n_features=[8, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train_state and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_state = create_train_state(hybrid_awbm, \n",
    "                                  main_key=main_key, \n",
    "                                  params_key=params_key, \n",
    "                                  lr=1e-2,\n",
    "                                  n_features=covariates.shape[-1])\n",
    "\n",
    "trained_hybrid_state, hybrid_metrics_history = train(hybrid_state, \n",
    "                                       covariates, targets, \n",
    "                                       n_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(hybrid_metrics_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(hybrid_awbm, covariates, targets, state=trained_hybrid_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performance of the hybrid model with the AWBM model (NSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall performance of the awbm vs hybrid model\n",
    "print('Compare Nash-Sutcliffe Efficiency (NSE) scores')\n",
    "print(f\"AWBM NSE: {metrics_history['nse'][-1]:.4f}\")\n",
    "print(f\"Hybrid NSE: {hybrid_metrics_history['nse'][-1]:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the parameters of the model \n",
    "Here we can see that the to see how they have been optimised to different values in the AWBM only and the Hybrid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWBM model optimisation\n",
    "trained_state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid model optimisation\n",
    "trained_hybrid_state.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it, folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
